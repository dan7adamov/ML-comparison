{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator as op\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "uspsTrain = np.genfromtxt('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\zip.train\\\\zip.train', delimiter = \" \", autostrip = True)\n",
    "uspsTest = np.genfromtxt('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\zip.test\\\\zip.test', delimiter = \" \", autostrip = True)\n",
    "usps = np.concatenate((uspsTrain, uspsTest))\n",
    "X_trainUspsFloat, X_testUspsFloat, y_trainUsps, y_testUsps = tts(usps[:,1:], usps[:, 0], test_size=0.125, random_state=709)\n",
    "# Dowloaded and parsed usps dataset into train and test datasets(ratio 7:1 respectively) and have separate arrays for features and their corresponding labels\n",
    "for k in range(X_trainUspsFloat.shape[0]):\n",
    "    X_trainUspsFloat[k]=(X_trainUspsFloat[k,:]*127.5+127.5)\n",
    "for k in range(X_testUspsFloat.shape[0]):\n",
    "    X_testUspsFloat[k]=(X_testUspsFloat[k,:]*127.5+127.5)\n",
    "X_trainUsps = X_trainUspsFloat.astype(int)\n",
    "X_testUsps = X_testUspsFloat.astype(int)\n",
    "# Normalised the arrays for the application (made values int between 0 and 255)\n",
    "\n",
    "\n",
    "X_trainMnist = np.array([])\n",
    "y_trainMnist = np.array([])\n",
    "X_testMnist = np.array([])\n",
    "y_testMnist = np.array([])\n",
    "with gzip.open('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\train-images-idx3-ubyte.gz', 'rb') as trainSampleFile:\n",
    "    trainSampleBuffer = trainSampleFile.read()\n",
    "    X_trainMnistUnshaped = np.frombuffer(trainSampleBuffer, dtype = np.uint8, offset = 16)\n",
    "    X_trainMnist = X_trainMnistUnshaped.reshape(60000, 784)\n",
    "with gzip.open('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\train-labels-idx1-ubyte.gz', 'rb') as trainLabelFile:\n",
    "    trainLabelBuffer = trainLabelFile.read()\n",
    "    y_trainMnist = np.frombuffer(trainLabelBuffer, dtype = np.uint8, offset = 8)\n",
    "with gzip.open('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\t10k-images-idx3-ubyte.gz', 'rb') as testSampleFile:\n",
    "    testSampleBuffer = testSampleFile.read()\n",
    "    X_testMnistUnshaped = np.frombuffer(testSampleBuffer, dtype = np.uint8, offset = 16)\n",
    "    X_testMnist = X_testMnistUnshaped.reshape(10000, 784)\n",
    "with gzip.open('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\Individual Project\\\\t10k-labels-idx1-ubyte.gz', 'rb') as testLabelFile:\n",
    "    testLabelBuffer = testLabelFile.read()\n",
    "    y_testMnist = np.frombuffer(testLabelBuffer, dtype = np.uint8, offset = 8)\n",
    "# Dowloaded Mnist dataset into train and test datasets(ratio 6:1 respectively) and have separate arrays for features and their corresponding labels\n",
    "\n",
    "# global variables\n",
    "treeLabels = None\n",
    "maxTreeDepth = 10\n",
    "\n",
    "# generates all possible tuples with different label combinations and makes an empty tree with it\n",
    "def treesGenerator():\n",
    "    global allTrees\n",
    "    allTrees = []\n",
    "    for i in range(10):\n",
    "        for j in range(i + 1, 10):\n",
    "            allTrees.append(TreeLabelWrapper((i, j)))\n",
    "\n",
    "# starts classifing samples and making divisions to form a complete DecisionTree\n",
    "def classifyAllTrees(X_train, y_train):\n",
    "    for t in allTrees:\n",
    "        t.treeFactory(X_train, y_train)\n",
    "\n",
    "def classifyFromTree(X_train, y_train, treeIndex, treeFinalIndex):\n",
    "    for i in range(treeIndex, treeFinalIndex+1):\n",
    "        allTrees[i].treeFactory(X_train, y_train)\n",
    "        pkl.dump( allTrees, open( \"allTrees-{0:0{1}}\".format(i, 3), \"wb\" ) )\n",
    "\n",
    "# returns the predicted label for a given sample, by looking at which label is most common from all possible Trees combinations\n",
    "def decisionMaker(sample):\n",
    "    predLabels = []\n",
    "    for t in allTrees:\n",
    "        predLabels.append( t.tree.classifierV2(sample) )\n",
    "    return max(set(predLabels), key = predLabels.count)\n",
    "\n",
    "# method for finding the error rate\n",
    "def errorRate(xTestSet, yTestSet):\n",
    "        numOfErrors = 0\n",
    "        for k in range(xTestSet.shape[0]): # testSet should be an np.array of rank 2\n",
    "            predLabel = decisionMaker(xTestSet[k])\n",
    "            if predLabel != yTestSet[k]:\n",
    "                numOfErrors += 1\n",
    "        return numOfErrors / xTestSet.shape[0]\n",
    "    \n",
    "\n",
    "# wrapper class facilitates generation of decision trees with different label tuples\n",
    "class TreeLabelWrapper:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.tree = DecisionTree()\n",
    "        \n",
    "    def treeFactory(self, X_train, y_train):\n",
    "        global treeLabels\n",
    "        treeLabels = self.labels\n",
    "        self.tree.treeFactory(X_train, y_train, [])\n",
    "        \n",
    "\n",
    "# main algorithm logic class. Creates instances of tree nodes, and hondles all the splitting\n",
    "class DecisionTree:\n",
    "    def __init__(self, featureNmbr = None, featureThreshold = None, predictedLabel = None):\n",
    "        self.featureNmbr = featureNmbr\n",
    "        self.featureThreshold = featureThreshold\n",
    "        self.predLabel = predictedLabel\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.subSetVolume = None # debug variable - number of samples in the current node\n",
    "        \n",
    "    # method for creating instances of DecisionTree which are nodes in our tree, and splitting the dataset into child nodes\n",
    "    def treeFactory(self, X_train, y_train, path):\n",
    "        if len(path) >= maxTreeDepth:\n",
    "            return\n",
    "        featNum, featThr, labelIndices, errorRate = self.featureSelector(X_train, y_train, path)\n",
    "        \n",
    "        # do not create 'empty' nodes with uninformative divisions, or inadequate number of samples in the subset\n",
    "        if featThr < 0 or featNum is None:            \n",
    "            return\n",
    "        \n",
    "        # divison is informative so two new nodes are created and their parent node contains the split information\n",
    "        self.featureNmbr, self.featureThreshold = featNum, featThr\n",
    "        self.right = DecisionTree(None, None, treeLabels[labelIndices[0]])\n",
    "        self.left = DecisionTree(None, None, treeLabels[labelIndices[1]])\n",
    "        self.right.treeFactory(X_train, y_train, path + [(featNum, featThr, op.__ge__)])\n",
    "        self.left.treeFactory(X_train, y_train, path + [(featNum, featThr, op.__lt__)])\n",
    "        \n",
    "    \n",
    "    # method for selecting the most informative threshold of a given feature number\n",
    "    def featureThresholdSelectorV3(self, X_train, y_train, featureNmbr, path):\n",
    "        \n",
    "        indicesOfTreeLabel = 0, 1\n",
    "        errorRate = 1\n",
    "        \n",
    "        # debug variable - number of samples in the current node\n",
    "        self.subSetVolume = 0\n",
    "        \n",
    "        instancesOfFeatureLabel = np.zeros(256)\n",
    "        instancesOfFeatureNotLabel = np.zeros(256)\n",
    "        \n",
    "        # throwing away samples which dont fit earlier constraints of the nodes\n",
    "        for i in range(X_train.shape[0]):\n",
    "            goodSample = True\n",
    "            if path:\n",
    "                for t in path: # t is a tuple -> (featureNumber, featureThreshold, operator)\n",
    "                    if not t[2] (X_train[i, t[0]], t[1]):\n",
    "                        goodSample = False\n",
    "                        break\n",
    "                if not goodSample:\n",
    "                    continue\n",
    "                    \n",
    "            # Processing only samples with 2 correct labels\n",
    "            if y_train[i] == treeLabels[0]:\n",
    "                instancesOfFeatureLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "            elif y_train[i] == treeLabels[1]:\n",
    "                instancesOfFeatureNotLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "        \n",
    "        # ammount of samples of each label type in the current node\n",
    "        sumLabel = np.sum(instancesOfFeatureLabel)\n",
    "        sumNotLabel = np.sum(instancesOfFeatureNotLabel)\n",
    "               \n",
    "        # uninformative splits are prevented\n",
    "        if sumLabel == 0 and sumNotLabel == 0: # test for when max depth is reached\n",
    "            # print(\"Possible Bug - no samples in subset or max depth is reached by the tree\")\n",
    "            return 0, (0, 1), 1.1\n",
    "        if sumLabel == 0: # nothing to split, only one type of labels\n",
    "            return -1, (1, 0), 1.1\n",
    "        if sumNotLabel == 0: # nothing to split, only one type of labels\n",
    "            return -2, (0, 1), 1.1\n",
    "        \n",
    "        # find the most common greyscale number\n",
    "        featureArgmax = np.argmax(instancesOfFeatureLabel)\n",
    "        notFeatureArgmax = np.argmax(instancesOfFeatureNotLabel)\n",
    "        \n",
    "        # find the mean of the two numbers, which is a threshold\n",
    "        featureThreshold = int(round((featureArgmax + notFeatureArgmax) / 2))\n",
    "        \n",
    "        # cumilitive array is calculated for finding error rate\n",
    "        cumSumFeature = np.cumsum(instancesOfFeatureLabel[::-1])[::-1]\n",
    "        cumSumNotFeature = np.cumsum(instancesOfFeatureNotLabel[::-1])[::-1]\n",
    "        \n",
    "        # the order of labels is decided and the error rate is given\n",
    "        if featureThreshold != 0:\n",
    "            if featureArgmax >= notFeatureArgmax:\n",
    "                indicesOfTreeLabel = 0, 1\n",
    "                errorRate = 1 - cumSumFeature[featureThreshold] / cumSumFeature[0]\n",
    "            else:\n",
    "                indicesOfTreeLabel = 1, 0\n",
    "                errorRate = 1 - cumSumNotFeature[featureThreshold] / cumSumNotFeature[0]\n",
    "\n",
    "        return featureThreshold, indicesOfTreeLabel, errorRate\n",
    "    \n",
    "    \n",
    "    # method for selecting a feature number with most informative division\n",
    "    def featureSelector(self, X_train, y_train, path):\n",
    "        leastErrorRateFeatureIndex = 0\n",
    "        leastErrorRateFeatureThreshold = 0\n",
    "        curFeatureLabelIndices = 0, 1\n",
    "        leastErrorRate = 1\n",
    "        \n",
    "        # iterates through all features\n",
    "        for featNbr in range(X_train.shape[1]):\n",
    "        # for featNbr in (300, 305, 310, 315, 320, 325, 330, 335, 340): # DEBUG\n",
    "        \n",
    "            # prevents algorithm from slecting the same feature number for two consecutive nodes\n",
    "            if path and featNbr != path[-1][0] or not path:\n",
    "                # featureThreshold, indicesOfTreeLabel, errorRate = curFeature\n",
    "                curFeature = self.featureThresholdSelectorV3(X_train, y_train, featNbr, path)\n",
    "                \n",
    "                # catches split which is not informative (featureThreshold = 0)\n",
    "                if curFeature[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                # node with a subset containing only one label\n",
    "                if curFeature[0] < 0:\n",
    "                    return None, curFeature[0], curFeature[1], 0\n",
    "                \n",
    "                # good splits are slowly improved, by selecting the feature number with least error rate\n",
    "                if curFeature[2] < leastErrorRate:\n",
    "                    leastErrorRateFeatureIndex = featNbr\n",
    "                    leastErrorRateFeatureThreshold = curFeature[0]\n",
    "                    curFeatureLabelIndices = curFeature[1]\n",
    "                    leastErrorRate = curFeature[2]\n",
    "                   \n",
    "        return leastErrorRateFeatureIndex, leastErrorRateFeatureThreshold, curFeatureLabelIndices, leastErrorRate\n",
    "    \n",
    "    \n",
    "    # method for getting a predicted label from a given sample from the Tree we built\n",
    "    def classifierV2(self, sample):\n",
    "        \n",
    "        # if that is a node with 100% accuracy (only samples of one label are left in this node)\n",
    "        if self.featureThreshold is None:\n",
    "            return self.predLabel\n",
    "        \n",
    "        # this is satisfied if this is the last node in the tree, but because of depth constraint, we cannot go any deeper\n",
    "        elif self.right is None and self.left is None:\n",
    "            return self.predLabel\n",
    "        \n",
    "        # recursion\n",
    "        else:\n",
    "            if sample[self.featureNmbr] >= self.featureThreshold:\n",
    "                return self.right.classifierV2(sample)\n",
    "            if sample[self.featureNmbr] < self.featureThreshold:\n",
    "                return self.left.classifierV2(sample)\n",
    "    \n",
    "    # debug function - prints all the tree information from its nodes\n",
    "    def auditFull(self, depth = 0):\n",
    "        print()\n",
    "        print(\"depth =\", depth)\n",
    "        if self.subSetVolume:\n",
    "            print(\"volume =\", self.subSetVolume)\n",
    "        print(self.featureNmbr, self.featureThreshold, self.predLabel)\n",
    "        for subTree in (self.left, self.right):\n",
    "            if subTree:\n",
    "                subTree.auditFull(depth + 1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "treesGenerator()\n",
    "classifyAllTrees(X_trainUsps[:1000], y_trainUsps[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRate(X_trainUsps[:1000], y_trainUsps[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3284608770421324"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRate(X_testUsps, y_testUsps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "depth = 0\n",
      "volume = 172\n",
      "200 128 None\n",
      "\n",
      "depth = 1\n",
      "volume = 46\n",
      "41 2 4\n",
      "\n",
      "depth = 2\n",
      "volume = 21\n",
      "101 56 4\n",
      "\n",
      "depth = 3\n",
      "volume = 8\n",
      "None None 4\n",
      "\n",
      "depth = 3\n",
      "volume = 13\n",
      "85 82 7\n",
      "\n",
      "depth = 4\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 4\n",
      "volume = 12\n",
      "None None 4\n",
      "\n",
      "depth = 2\n",
      "volume = 25\n",
      "57 10 7\n",
      "\n",
      "depth = 3\n",
      "volume = 4\n",
      "None None 7\n",
      "\n",
      "depth = 3\n",
      "volume = 21\n",
      "89 12 4\n",
      "\n",
      "depth = 4\n",
      "volume = 4\n",
      "None None 7\n",
      "\n",
      "depth = 4\n",
      "volume = 17\n",
      "4 10 4\n",
      "\n",
      "depth = 5\n",
      "volume = 12\n",
      "None None 4\n",
      "\n",
      "depth = 5\n",
      "volume = 5\n",
      "5 102 7\n",
      "\n",
      "depth = 6\n",
      "volume = 1\n",
      "None None 4\n",
      "\n",
      "depth = 6\n",
      "volume = 4\n",
      "4 51 7\n",
      "\n",
      "depth = 7\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 7\n",
      "volume = 3\n",
      "3 2 4\n",
      "\n",
      "depth = 8\n",
      "volume = 1\n",
      "None None 4\n",
      "\n",
      "depth = 8\n",
      "volume = 2\n",
      "None None 7\n",
      "\n",
      "depth = 1\n",
      "volume = 126\n",
      "26 128 7\n",
      "\n",
      "depth = 2\n",
      "volume = 41\n",
      "122 128 4\n",
      "\n",
      "depth = 3\n",
      "volume = 9\n",
      "24 12 7\n",
      "\n",
      "depth = 4\n",
      "volume = 2\n",
      "None None 7\n",
      "\n",
      "depth = 4\n",
      "volume = 7\n",
      "5 29 4\n",
      "\n",
      "depth = 5\n",
      "volume = 5\n",
      "None None 4\n",
      "\n",
      "depth = 5\n",
      "volume = 2\n",
      "None None 7\n",
      "\n",
      "depth = 3\n",
      "volume = 32\n",
      "59 40 4\n",
      "\n",
      "depth = 4\n",
      "volume = 6\n",
      "None None 4\n",
      "\n",
      "depth = 4\n",
      "volume = 26\n",
      "106 137 7\n",
      "\n",
      "depth = 5\n",
      "volume = 3\n",
      "None None 7\n",
      "\n",
      "depth = 5\n",
      "volume = 23\n",
      "5 2 4\n",
      "\n",
      "depth = 6\n",
      "volume = 11\n",
      "None None 4\n",
      "\n",
      "depth = 6\n",
      "volume = 12\n",
      "28 7 7\n",
      "\n",
      "depth = 7\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 7\n",
      "volume = 11\n",
      "5 4 4\n",
      "\n",
      "depth = 8\n",
      "volume = 1\n",
      "None None 4\n",
      "\n",
      "depth = 8\n",
      "volume = 10\n",
      "27 32 7\n",
      "\n",
      "depth = 9\n",
      "volume = 1\n",
      "None None 4\n",
      "\n",
      "depth = 9\n",
      "volume = 9\n",
      "5 6 7\n",
      "\n",
      "depth = 10\n",
      "None None 7\n",
      "\n",
      "depth = 10\n",
      "None None 4\n",
      "\n",
      "depth = 2\n",
      "volume = 85\n",
      "89 128 7\n",
      "\n",
      "depth = 3\n",
      "volume = 59\n",
      "None None 7\n",
      "\n",
      "depth = 3\n",
      "volume = 26\n",
      "23 128 4\n",
      "\n",
      "depth = 4\n",
      "volume = 15\n",
      "None None 4\n",
      "\n",
      "depth = 4\n",
      "volume = 11\n",
      "6 29 7\n",
      "\n",
      "depth = 5\n",
      "volume = 2\n",
      "None None 7\n",
      "\n",
      "depth = 5\n",
      "volume = 9\n",
      "5 4 4\n",
      "\n",
      "depth = 6\n",
      "volume = 1\n",
      "None None 4\n",
      "\n",
      "depth = 6\n",
      "volume = 8\n",
      "None None 7\n"
     ]
    }
   ],
   "source": [
    "allTrees[32].tree.auditFull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(allTrees, open(\"decisionTreeModelUSPS.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrees = pkl.load(open(\"decisionTreeModelUSPS.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_testMnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-313f507db6f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0my_testMnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0minstance\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestWrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifierV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testMnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_testMnist' is not defined"
     ]
    }
   ],
   "source": [
    "instance = 0\n",
    "error = 0\n",
    "for k in range(500):\n",
    "    if y_testMnist[k] in [2, 7]:\n",
    "        instance += 1\n",
    "        pred = testWrapper.tree.classifierV2(X_testMnist[k])\n",
    "        if pred != y_testMnist[k]:\n",
    "            error += 1\n",
    "            # plt.imshow((X_trainMnist[k,:]).astype(int).reshape(28,28))\n",
    "            print(\"k\", k, \"prediction:\", pred, \"label:\", y_testMnist[k])\n",
    "print(error/instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "testWrapper = TreeLabelWrapper( (2, 7) )\n",
    "testWrapper.treeFactory(X_trainMnist[:300], y_trainMnist[:300])\n",
    "\n",
    "# k = 117\n",
    "for k in range(100, 250):\n",
    "    if y_trainMnist[k] in [2, 7]:\n",
    "        pred = testWrapper.tree.classifierV2(X_trainMnist[k])\n",
    "        if pred != y_trainMnist[k]:\n",
    "            # plt.imshow((X_trainMnist[k,:]).astype(int).reshape(28,28))\n",
    "            print(\"k\", k, \"prediction:\", pred, \"label:\", y_trainMnist[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
