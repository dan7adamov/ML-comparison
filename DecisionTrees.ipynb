{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator as op\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# uspsTrain = np.genfromtxt('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\CS3920\\\\zip.train\\\\zip.train', delimiter = \" \", autostrip = True)\n",
    "# uspsTest = np.genfromtxt('C:\\\\Users\\\\Dan Adamov\\\\Desktop\\\\RHUL\\\\3rd Year\\\\CS3920\\\\zip.test\\\\zip.test', delimiter = \" \", autostrip = True)\n",
    "# usps = np.concatenate((uspsTrain, uspsTest))\n",
    "# X_trainUsps, X_testUsps, y_trainUsps, y_testUsps = tts(usps[:,1:], usps[:, 0], test_size=0.125, random_state=709)\n",
    "# Dowloaded and parsed usps dataset into train and test datasets(ratio 8:1 respectively) and have separate arrays for features and their corresponding labels\n",
    "\n",
    "X_trainMnist = np.array([])\n",
    "y_trainMnist = np.array([])\n",
    "X_testMnist = np.array([])\n",
    "y_testMnist = np.array([])\n",
    "with gzip.open('C:\\\\Users\\\\danad\\\\Personal Project\\\\Individual Project\\\\train-images-idx3-ubyte.gz', 'rb') as trainSampleFile:\n",
    "    trainSampleBuffer = trainSampleFile.read()\n",
    "    X_trainMnistUnshaped = np.frombuffer(trainSampleBuffer, dtype = np.uint8, offset = 16)\n",
    "    X_trainMnist = X_trainMnistUnshaped.reshape(60000, 784)\n",
    "with gzip.open('C:\\\\Users\\\\danad\\\\Personal Project\\\\Individual Project\\\\train-labels-idx1-ubyte.gz', 'rb') as trainLabelFile:\n",
    "    trainLabelBuffer = trainLabelFile.read()\n",
    "    y_trainMnist = np.frombuffer(trainLabelBuffer, dtype = np.uint8, offset = 8)\n",
    "with gzip.open('C:\\\\Users\\\\danad\\\\Personal Project\\\\Individual Project\\\\t10k-images-idx3-ubyte.gz', 'rb') as testSampleFile:\n",
    "    testSampleBuffer = testSampleFile.read()\n",
    "    X_testMnistUnshaped = np.frombuffer(testSampleBuffer, dtype = np.uint8, offset = 16)\n",
    "    X_testMnist = X_testMnistUnshaped.reshape(10000, 784)\n",
    "with gzip.open('C:\\\\Users\\\\danad\\\\Personal Project\\\\Individual Project\\\\t10k-labels-idx1-ubyte.gz', 'rb') as testLabelFile:\n",
    "    testLabelBuffer = testLabelFile.read()\n",
    "    y_testMnist = np.frombuffer(testLabelBuffer, dtype = np.uint8, offset = 8)\n",
    "# Dowloaded Mnist dataset into train and test datasets(ratio 6:1 respectively) and have separate arrays for features and their corresponding labels\n",
    "\n",
    "# global variables\n",
    "treeLabels = None\n",
    "maxTreeDepth = 10\n",
    "\n",
    "# generates all possible tuples with different label combinations and makes an empty tree with it\n",
    "def treesGenerator():\n",
    "    global allTrees\n",
    "    allTrees = []\n",
    "    for i in range(10):\n",
    "        for j in range(i + 1, 10):\n",
    "            allTrees.append(TreeLabelWrapper((i, j)))\n",
    "\n",
    "# starts classifing samples and making divisions to form a complete DecisionTree\n",
    "def classifyAllTrees(X_train, y_train):\n",
    "    for t in allTrees:\n",
    "        t.treeFactory(X_train, y_train)\n",
    "\n",
    "# returns the predicted label for a given sample, by looking at which label is most common from all possible Trees combinations\n",
    "def decisionMaker(sample):\n",
    "    predLabels = []\n",
    "    for t in allTrees:\n",
    "        predLabels.append( t.tree.classifierV2(sample) )\n",
    "    return max(set(predLabels), key = predLabels.count)\n",
    "\n",
    "# method for finding the error rate\n",
    "def errorRate(xTestSet, yTestSet):\n",
    "        numOfErrors = 0\n",
    "        for k in range(xTestSet.shape[0]): # testSet should be an np.array of rank 2\n",
    "            predLabel = decisionMaker(xTestSet[k])\n",
    "            if predLabel != yTestSet[k]:\n",
    "                numOfErrors += 1\n",
    "        return numOfErrors / xTestSet.shape[0]\n",
    "    \n",
    "\n",
    "# wrapper class facilitates generation of decision trees with different label tuples\n",
    "class TreeLabelWrapper:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.tree = DecisionTree()\n",
    "        \n",
    "    def treeFactory(self, X_train, y_train):\n",
    "        global treeLabels\n",
    "        treeLabels = self.labels\n",
    "        self.tree.treeFactory(X_train, y_train, [])\n",
    "        \n",
    "\n",
    "# main algorithm logic class. Creates instances of tree nodes, and hondles all the splitting\n",
    "class DecisionTree:\n",
    "    def __init__(self, featureNmbr = None, featureThreshold = None, predictedLabel = None):\n",
    "        self.featureNmbr = featureNmbr\n",
    "        self.featureThreshold = featureThreshold\n",
    "        self.predLabel = predictedLabel\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.subSetVolume = None # debug variable - number of samples in the current node\n",
    "        \n",
    "    # method for creating instances of DecisionTree which are nodes in our tree, and splitting the dataset into child nodes\n",
    "    def treeFactory(self, X_train, y_train, path):\n",
    "        if len(path) >= maxTreeDepth:\n",
    "            return\n",
    "        featNum, featThr, labelIndices, errorRate = self.featureSelector(X_train, y_train, path)\n",
    "        \n",
    "        # do not create 'empty' nodes with uninformative divisions, or inadequate number of samples in the subset\n",
    "        if featThr < 0 or featNum is None:            \n",
    "            return\n",
    "        \n",
    "        # divison is informative so two new nodes are created and their parent node contains the split information\n",
    "        self.featureNmbr, self.featureThreshold = featNum, featThr\n",
    "        self.right = DecisionTree(None, None, treeLabels[labelIndices[0]])\n",
    "        self.left = DecisionTree(None, None, treeLabels[labelIndices[1]])\n",
    "        self.right.treeFactory(X_train, y_train, path + [(featNum, featThr, op.__ge__)])\n",
    "        self.left.treeFactory(X_train, y_train, path + [(featNum, featThr, op.__lt__)])\n",
    "    \n",
    "    \n",
    "    # method for selecting the most informative threshold of a given feature number\n",
    "    def featureThresholdSelectorV2(self, X_train, y_train, featureNmbr, path):\n",
    "        \n",
    "        indicesOfTreeLabel = 0, 1\n",
    "        errorRate = 1\n",
    "        \n",
    "        # debug variable - number of samples in the current node\n",
    "        self.subSetVolume = 0\n",
    "        \n",
    "        instancesOfFeatureLabel = np.zeros(256)\n",
    "        instancesOfFeatureNotLabel = np.zeros(256)\n",
    "        cumulativeThreshold = np.zeros(256)\n",
    "        \n",
    "        # throwing away samples which dont fit earlier constraints of the nodes\n",
    "        for i in range(X_train.shape[0]):\n",
    "            goodSample = True\n",
    "            if path:\n",
    "                for t in path: # t is a tuple -> (featureNumber, featureThreshold, operator)\n",
    "                    if not t[2] (X_train[i, t[0]], t[1]):\n",
    "                        goodSample = False\n",
    "                        break\n",
    "                if not goodSample:\n",
    "                    continue\n",
    "                    \n",
    "            # Processing only samples with 2 correct labels\n",
    "            if y_train[i] == treeLabels[0]:\n",
    "                instancesOfFeatureLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "            elif y_train[i] == treeLabels[1]:\n",
    "                instancesOfFeatureNotLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "        \n",
    "        # two arrays counting number of samples which get activated for each of the thresholds\n",
    "        cumSumFeature = np.cumsum(instancesOfFeatureLabel[::-1])[::-1]\n",
    "        cumSumNotFeature = np.cumsum(instancesOfFeatureNotLabel[::-1])[::-1]\n",
    "        \n",
    "        # uninformative splits are prevented\n",
    "        if cumSumFeature[0] == 0 and cumSumNotFeature[0] == 0: # Extra test, should never happen\n",
    "            print(\"Possible Bug - no samples in subset\")\n",
    "            return 0, (0, 1), 1.1\n",
    "        if cumSumFeature[0] == 0: # nothing to split, only one type of labels\n",
    "            return -1, (1, 0), 1.1\n",
    "        if cumSumNotFeature[0] == 0: # nothing to split, only one type of labels\n",
    "            return -2, (0, 1), 1.1\n",
    "        \n",
    "        # the threshold is calculated, by taking the number that gets a division with highest accuracy\n",
    "        np.subtract(cumSumFeature, cumSumNotFeature, cumulativeThreshold)\n",
    "        featureThreshold = np.argmax(abs(cumulativeThreshold))\n",
    "        \n",
    "        # the order of labels is decided and the error rate is given\n",
    "        if featureThreshold != 0:\n",
    "            if cumulativeThreshold[featureThreshold] >= 0:\n",
    "                indicesOfTreeLabel = 0, 1\n",
    "                errorRate = 1 - cumSumFeature[featureThreshold] / cumSumFeature[0]\n",
    "            else:\n",
    "                indicesOfTreeLabel = 1, 0\n",
    "                errorRate = 1 - cumSumNotFeature[featureThreshold] / cumSumNotFeature[0]\n",
    "\n",
    "        return featureThreshold, indicesOfTreeLabel, errorRate\n",
    "    \n",
    "    # method for selecting the most informative threshold of a given feature number\n",
    "    def featureThresholdSelectorV3(self, X_train, y_train, featureNmbr, path):\n",
    "        \n",
    "        indicesOfTreeLabel = 0, 1\n",
    "        errorRate = 1\n",
    "        \n",
    "        # debug variable - number of samples in the current node\n",
    "        self.subSetVolume = 0\n",
    "        \n",
    "        instancesOfFeatureLabel = np.zeros(256)\n",
    "        instancesOfFeatureNotLabel = np.zeros(256)\n",
    "        cumulativeThreshold = np.zeros(256)\n",
    "        \n",
    "        # throwing away samples which dont fit earlier constraints of the nodes\n",
    "        for i in range(X_train.shape[0]):\n",
    "            goodSample = True\n",
    "            if path:\n",
    "                for t in path: # t is a tuple -> (featureNumber, featureThreshold, operator)\n",
    "                    if not t[2] (X_train[i, t[0]], t[1]):\n",
    "                        goodSample = False\n",
    "                        break\n",
    "                if not goodSample:\n",
    "                    continue\n",
    "                    \n",
    "            # Processing only samples with 2 correct labels\n",
    "            if y_train[i] == treeLabels[0]:\n",
    "                instancesOfFeatureLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "            elif y_train[i] == treeLabels[1]:\n",
    "                instancesOfFeatureNotLabel[X_train[i,featureNmbr]] += 1\n",
    "                self.subSetVolume += 1\n",
    "        \n",
    "        # ammount of samples of each label type\n",
    "        sumLabel = np.sum(instancesOfFeatureLabel)\n",
    "        sumNotLabel = np.sum(instancesOfFeatureNotLabel)\n",
    "               \n",
    "        # uninformative splits are prevented\n",
    "        if sumLabel == 0 and sumNotLabel == 0: # Extra test, should never happen\n",
    "            print(\"Possible Bug - no samples in subset\")\n",
    "            return 0, (0, 1), 1.1\n",
    "        if sumLabel == 0: # nothing to split, only one type of labels\n",
    "            return -1, (1, 0), 1.1\n",
    "        if sumNotLabel == 0: # nothing to split, only one type of labels\n",
    "            return -2, (0, 1), 1.1\n",
    "        \n",
    "        # find the most common greyscale number\n",
    "        featureArgmax = np.argmax(instancesOfFeatureLabel)\n",
    "        notFeatureArgmax = np.argmax(instancesOfFeatureNotLabel)\n",
    "        \n",
    "        # find the mean of the two numbers, which is a threshold\n",
    "        featureThreshold = round((featureArgmax + notFeatureArgmax) / 2)\n",
    "        \n",
    "        # cumilitive array is calculated for finding error rate\n",
    "        cumSumFeature = np.cumsum(instancesOfFeatureLabel[::-1])[::-1]\n",
    "        cumSumNotFeature = np.cumsum(instancesOfFeatureNotLabel[::-1])[::-1]\n",
    "        \n",
    "        # the order of labels is decided and the error rate is given\n",
    "        if featureThreshold != 0:\n",
    "            if featureArgmax >= notFeatureArgmax:\n",
    "                indicesOfTreeLabel = 0, 1\n",
    "                errorRate = 1 - cumSumFeature[featureThreshold] / cumSumFeature[0]\n",
    "            else:\n",
    "                indicesOfTreeLabel = 1, 0\n",
    "                errorRate = 1 - cumSumNotFeature[featureThreshold] / cumSumNotFeature[0]\n",
    "\n",
    "        return featureThreshold, indicesOfTreeLabel, errorRate\n",
    "    \n",
    "    \n",
    "    # method for selecting a feature number with most informative division\n",
    "    def featureSelector(self, X_train, y_train, path):\n",
    "        leastErrorRateFeatureIndex = 0\n",
    "        leastErrorRateFeatureThreshold = 0\n",
    "        curFeatureLabelIndices = 0, 1\n",
    "        leastErrorRate = 1\n",
    "        \n",
    "        # iterates through all features\n",
    "        for featNbr in range(X_train.shape[1]):\n",
    "        # for featNbr in (300, 305, 310, 315, 320, 325, 330, 335, 340): # DEBUG\n",
    "        \n",
    "            # prevents algorithm from slecting the same feature number for two consecutive nodes\n",
    "            if path and featNbr != path[-1][0] or not path:\n",
    "                # featureThreshold, indicesOfTreeLabel, errorRate = curFeature\n",
    "                curFeature = self.featureThresholdSelectorV3(X_train, y_train, featNbr, path)\n",
    "                \n",
    "                # catches split which is not informative (featureThreshold = 0)\n",
    "                if curFeature[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                # node with a subset containing only one label\n",
    "                if curFeature[0] < 0:\n",
    "                    return None, curFeature[0], curFeature[1], 0\n",
    "                \n",
    "                # good splits are slowly improved, by selecting the feature number with least error rate\n",
    "                if curFeature[2] < leastErrorRate:\n",
    "                    leastErrorRateFeatureIndex = featNbr\n",
    "                    leastErrorRateFeatureThreshold = curFeature[0]\n",
    "                    curFeatureLabelIndices = curFeature[1]\n",
    "                    leastErrorRate = curFeature[2]\n",
    "                   \n",
    "        return leastErrorRateFeatureIndex, leastErrorRateFeatureThreshold, curFeatureLabelIndices, leastErrorRate\n",
    "    \n",
    "    \n",
    "    # method for getting a predicted label from a given sample from the Tree we built\n",
    "    def classifierV2(self, sample):\n",
    "        \n",
    "        # if that is a node with 100% accuracy (only samples of one label are left in this node)\n",
    "        if self.featureThreshold is None:\n",
    "            return self.predLabel\n",
    "        \n",
    "        # this is satisfied if this is the last node in the tree, but because of depth constraint, we cannot go any deeper\n",
    "        elif self.right is None and self.left is None:\n",
    "            return self.predLabel\n",
    "        \n",
    "        # recursion\n",
    "        else:\n",
    "            if sample[self.featureNmbr] >= self.featureThreshold:\n",
    "                return self.right.classifierV2(sample)\n",
    "            if sample[self.featureNmbr] < self.featureThreshold:\n",
    "                return self.left.classifierV2(sample)\n",
    "    \n",
    "    # debug function - prints all the tree information from its nodes\n",
    "    def auditFull(self, depth = 0):\n",
    "        print()\n",
    "        print(\"depth =\", depth)\n",
    "        if self.subSetVolume:\n",
    "            print(\"volume =\", self.subSetVolume)\n",
    "        print(self.featureNmbr, self.featureThreshold, self.predLabel)\n",
    "        for subTree in (self.left, self.right):\n",
    "            if subTree:\n",
    "                subTree.auditFull(depth + 1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "testWrapper = TreeLabelWrapper( (2, 7) )\n",
    "testWrapper.treeFactory(X_trainMnist[:300], y_trainMnist[:300])\n",
    "\n",
    "# k = 117\n",
    "for k in range(100, 250):\n",
    "    if y_trainMnist[k] in [2, 7]:\n",
    "        pred = testWrapper.tree.classifierV2(X_trainMnist[k])\n",
    "        if pred != y_trainMnist[k]:\n",
    "            # plt.imshow((X_trainMnist[k,:]).astype(int).reshape(28,28))\n",
    "            print(\"k\", k, \"prediction:\", pred, \"label:\", y_trainMnist[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 34 prediction: 2 label: 7\n",
      "k 35 prediction: 7 label: 2\n",
      "k 38 prediction: 7 label: 2\n",
      "k 41 prediction: 2 label: 7\n",
      "k 80 prediction: 2 label: 7\n",
      "k 97 prediction: 2 label: 7\n",
      "k 111 prediction: 2 label: 7\n",
      "k 114 prediction: 2 label: 7\n",
      "k 119 prediction: 7 label: 2\n",
      "k 124 prediction: 2 label: 7\n",
      "k 147 prediction: 7 label: 2\n",
      "k 222 prediction: 7 label: 2\n",
      "k 229 prediction: 2 label: 7\n",
      "k 243 prediction: 2 label: 7\n",
      "k 254 prediction: 2 label: 7\n",
      "k 255 prediction: 2 label: 7\n",
      "k 263 prediction: 2 label: 7\n",
      "k 278 prediction: 7 label: 2\n",
      "k 282 prediction: 2 label: 7\n",
      "k 291 prediction: 7 label: 2\n",
      "k 303 prediction: 7 label: 2\n",
      "k 321 prediction: 7 label: 2\n",
      "k 370 prediction: 2 label: 7\n",
      "k 383 prediction: 2 label: 7\n",
      "k 404 prediction: 7 label: 2\n",
      "k 415 prediction: 2 label: 7\n",
      "k 421 prediction: 7 label: 2\n",
      "k 438 prediction: 2 label: 7\n",
      "k 444 prediction: 7 label: 2\n",
      "k 467 prediction: 7 label: 2\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7aa4785084f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;31m# plt.imshow((X_trainMnist[k,:]).astype(int).reshape(28,28))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"k\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"prediction:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_testMnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for k in range(500):\n",
    "    instance = 0\n",
    "    error = 0\n",
    "    if y_testMnist[k] in [2, 7]:\n",
    "        instance += 1\n",
    "        pred = testWrapper.tree.classifierV2(X_testMnist[k])\n",
    "        if pred != y_testMnist[k]:\n",
    "            error += 1\n",
    "            # plt.imshow((X_trainMnist[k,:]).astype(int).reshape(28,28))\n",
    "            print(\"k\", k, \"prediction:\", pred, \"label:\", y_testMnist[k])\n",
    "print(instance/error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "depth = 0\n",
      "volume = 57\n",
      "269 126 None\n",
      "\n",
      "depth = 1\n",
      "volume = 17\n",
      "261 19 2\n",
      "\n",
      "depth = 2\n",
      "volume = 8\n",
      "None None 2\n",
      "\n",
      "depth = 2\n",
      "volume = 9\n",
      "208 58 7\n",
      "\n",
      "depth = 3\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 3\n",
      "volume = 8\n",
      "209 146 2\n",
      "\n",
      "depth = 4\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 4\n",
      "volume = 7\n",
      "208 134 2\n",
      "\n",
      "depth = 5\n",
      "volume = 1\n",
      "None None 2\n",
      "\n",
      "depth = 5\n",
      "volume = 6\n",
      "181 16 7\n",
      "\n",
      "depth = 6\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 6\n",
      "volume = 5\n",
      "179 1 2\n",
      "\n",
      "depth = 7\n",
      "volume = 1\n",
      "None None 2\n",
      "\n",
      "depth = 7\n",
      "volume = 4\n",
      "180 120 7\n",
      "\n",
      "depth = 8\n",
      "volume = 1\n",
      "None None 2\n",
      "\n",
      "depth = 8\n",
      "volume = 3\n",
      "124 42 7\n",
      "\n",
      "depth = 9\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 9\n",
      "volume = 2\n",
      "None None 2\n",
      "\n",
      "depth = 1\n",
      "volume = 40\n",
      "267 126 7\n",
      "\n",
      "depth = 2\n",
      "volume = 9\n",
      "183 30 2\n",
      "\n",
      "depth = 3\n",
      "volume = 2\n",
      "None None 2\n",
      "\n",
      "depth = 3\n",
      "volume = 7\n",
      "155 8 7\n",
      "\n",
      "depth = 4\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 4\n",
      "volume = 6\n",
      "153 24 2\n",
      "\n",
      "depth = 5\n",
      "volume = 1\n",
      "None None 2\n",
      "\n",
      "depth = 5\n",
      "volume = 5\n",
      "152 11 7\n",
      "\n",
      "depth = 6\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 6\n",
      "volume = 4\n",
      "None None 2\n",
      "\n",
      "depth = 2\n",
      "volume = 31\n",
      "212 126 7\n",
      "\n",
      "depth = 3\n",
      "volume = 19\n",
      "None None 7\n",
      "\n",
      "depth = 3\n",
      "volume = 12\n",
      "214 15 2\n",
      "\n",
      "depth = 4\n",
      "volume = 2\n",
      "None None 2\n",
      "\n",
      "depth = 4\n",
      "volume = 10\n",
      "210 30 7\n",
      "\n",
      "depth = 5\n",
      "volume = 1\n",
      "None None 7\n",
      "\n",
      "depth = 5\n",
      "volume = 9\n",
      "214 89 2\n",
      "\n",
      "depth = 6\n",
      "volume = 2\n",
      "None None 2\n",
      "\n",
      "depth = 6\n",
      "volume = 7\n",
      "149 4 7\n",
      "\n",
      "depth = 7\n",
      "volume = 3\n",
      "None None 7\n",
      "\n",
      "depth = 7\n",
      "volume = 4\n",
      "None None 2\n"
     ]
    }
   ],
   "source": [
    "testWrapper.tree.auditFull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "treesGenerator()\n",
    "classifyAllTrees(X_trainMnist[:200], y_trainMnist[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate is 0.6565656565656566\n"
     ]
    }
   ],
   "source": [
    "correctPred = 0\n",
    "for n in range(len(y_testMnist[:100])):\n",
    "    if decisionMaker(X_testMnist[n]) == y_testMnist[n]:\n",
    "        correctPred += 1\n",
    "    # print()\n",
    "    # print(decisionMaker(X_testMnist[n]))\n",
    "    # print(y_testMnist[n])\n",
    "print(\"Error rate is\", correctPred/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRate(X_trainMnist[195:201], y_trainMnist[195:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRate(X_testMnist[:100], y_testMnist[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
